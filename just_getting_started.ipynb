{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([2,1])\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0.2140, 0.2891, 0.5446, 0.5765, 0.3228],\n",
      "        [0.6005, 0.0938, 0.8710, 0.7317, 0.8816]])\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "x= torch.zeros([2,5])\n",
    "print (x)\n",
    "y = torch.rand([2,5])\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor is basically a multidimensional array.\n",
    "### Torch is pretty similar to numpy when it come to manipulating multidimensional arrays/matrices. \n",
    "### Torch treats a model as a class.\n",
    "### One of the interesting things where most people get stuck is how to resize the tensor.\n",
    "### We use the resize() function in numpy and tensorflow to resize the multidimensional array.\n",
    "### This is not the case in pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2769, 0.1301, 0.8992, 0.4851, 0.8684],\n",
      "        [0.9770, 0.4341, 0.1683, 0.1569, 0.2959]]) torch.Size([2, 5])\n",
      "\n",
      " tensor([[0.2769, 0.1301, 0.8992, 0.4851, 0.8684, 0.9770, 0.4341, 0.1683, 0.1569,\n",
      "         0.2959]]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "y= torch.rand([2,5])\n",
    "# in order to change the shape of the tensor\n",
    "print(y, y.shape)\n",
    "new_y = y.view([1,10])\n",
    "print (\"\\n\",new_y, new_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing data analysis on the mnist dataset\n",
    "from torchvision import transforms, datasets\n",
    "''' essential that we feed the neural network with out of sample data because if by chance your model overfits then it will cause a high variance problem and lead to inaccuracy''' \n",
    "train = datasets.MNIST(\"\",train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test =  datasets.MNIST(\"\",train=False,download=True, transform=transforms.Compose([transforms.ToTensor()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: \n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: \n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchsize is how much samples we want to pass to our model at one time\n",
    "# we cannot pass the whole data to our model in one go \n",
    "# we eventually hope that our data will generslize and we can get important insights from it \n",
    "# the data is passed through multiple hidden layers the neuron a mini function who's coefficient has to be learned along the way gets updated. \n",
    "# mnist 28*28 pixel size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test , batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in trainset:\n",
    "    k= data\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "### This is where people get confused. First we can flatten or reshape our tensor using the view() function unlike the reshape function which is present in tensorflow and numpy \n",
    "### When we load our dataset trainset and divide it into different bathes of size 10 (so that model does not generalize in one step and we optimize it by decreasing the variance by training the model in multiple steps) we are essentially storing the pixel data in the first position and the image labels in the second position.\n",
    "\n",
    "#### -> When we are loading the dataset we have need to take care of the array dimension while loading the dataset the dimension looks like this (batch_size ,1, input_x_dim , input_y_dim ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image pixel data is stored in the zeroith position \n",
    "k[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image label data is stored in the first postiion \n",
    "k[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) tensor(5)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0] , data[1][0]\n",
    "print(x.shape,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8dc31a20d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# while viewing the image it has to be reshaped\n",
    "plt.imshow(x.view((28,28)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-set Balancing \n",
    "\n",
    "### If the model can find a shorter path to figuring out some- decreasing loss ( the model has no clue/knowledge of what the lowest loss could be, so as the optimizer is trying to decrease our loss it doesn't know how good the model might get it will just try to decrease the loss as best and easy  as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is up to date with 'origin/master'.\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n",
      "\r\n",
      "\t\u001b[31mmodified:   just_getting_started.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\r\n",
      "\t\u001b[31mMNIST/\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpytorch",
   "language": "python",
   "name": "cpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
