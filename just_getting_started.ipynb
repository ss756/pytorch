{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([2,1])\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0.9693, 0.7161, 0.2011, 0.3645, 0.2569],\n",
      "        [0.9395, 0.1581, 0.4712, 0.0783, 0.4440]])\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "x= torch.zeros([2,5])\n",
    "print (x)\n",
    "y = torch.rand([2,5])\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensor is basically a multidimensional array.\n",
    "* Torch is pretty similar to numpy when it come to manipulating multidimensional arrays/matrices. \n",
    "* Torch treats a model as a class.\n",
    "* One of the interesting things where most people get stuck is how to resize the tensor.\n",
    "* We use the resize() function in numpy and tensorflow to resize the multidimensional array.\n",
    "* This is not the case in pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5777, 0.2250, 0.0866, 0.0888, 0.8390],\n",
      "        [0.8251, 0.9423, 0.3129, 0.4161, 0.2985]]) torch.Size([2, 5])\n",
      "\n",
      " tensor([[0.5777, 0.2250, 0.0866, 0.0888, 0.8390, 0.8251, 0.9423, 0.3129, 0.4161,\n",
      "         0.2985]]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "y= torch.rand([2,5])\n",
    "# in order to change the shape of the tensor\n",
    "print(y, y.shape)\n",
    "new_y = y.view([1,10])\n",
    "print (\"\\n\",new_y, new_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing data analysis on the mnist dataset\n",
    "from torchvision import transforms, datasets\n",
    "''' essential that we feed the neural network with out of sample data because if by chance your model overfits then it will cause a high variance problem and lead to inaccuracy''' \n",
    "#train = datasets.MNIST(\"\",train=True,download = True,transform=transforms.Compose([transforms.ToTensor()]))\n",
    "#test =  datasets.MNIST(\"\",train=False, download = True,transform=transforms.Compose([transforms.ToTensor()])) \n",
    "train = datasets.MNIST(\"\",train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test =  datasets.MNIST(\"\",train=False, transform=transforms.Compose([transforms.ToTensor()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: \n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: \n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batchsize is how much samples we want to pass to our model at one time\n",
    "* we cannot pass the whole data to our model in one go \n",
    "* we eventually hope that our data will generslize and we can get important insights from it \n",
    "* the data is passed through multiple hidden layers the neuron a mini function who's coefficient has to be learned along the way gets updated. \n",
    "* mnist 28*28 pixel size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test , batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in trainset:\n",
    "    k= data\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "### This is where people get confused. First we can flatten or reshape our tensor using the view() function unlike the reshape function which is present in tensorflow and numpy \n",
    "### When we load our dataset trainset and divide it into different bathes of size 10 (so that model does not generalize in one step and we optimize it by decreasing the variance by training the model in multiple steps) we are essentially storing the pixel data in the first position and the image labels in the second position.\n",
    "\n",
    "* When we are loading the dataset we have need to take care of the array dimension while loading the dataset the dimension looks like this (batch_size ,1, input_x_dim , input_y_dim ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image pixel data is stored in the zeroith position \n",
    "k[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image label data is stored in the first postiion \n",
    "k[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) tensor(5)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0] , data[1][0]\n",
    "print(x.shape,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fce83265f70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# while viewing the image it has to be reshaped\n",
    "plt.imshow(x.view((28,28)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-set Balancing and Exploratory Data Analysis...\n",
    "\n",
    "### If the model can find a shorter path to figuring out some- decreasing loss ( the model has no clue/knowledge of what the lowest loss could be, so as the optimizer is trying to decrease our loss it doesn't know how good the model might get it will just try to decrease the loss as best and easy  as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total =0 \n",
    "counter =0\n",
    "batch_size = 10 \n",
    "counter_dict =dict()\n",
    "for data in trainset:\n",
    "    xs, ys = data\n",
    "    for y in ys:\n",
    "        if int(y) not in counter_dict:\n",
    "            counter_dict[int(y)]=1\n",
    "        else:\n",
    "            counter_dict[int(y)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: 5918,\n",
       " 0: 5923,\n",
       " 5: 5421,\n",
       " 1: 6742,\n",
       " 4: 5842,\n",
       " 8: 5851,\n",
       " 9: 5949,\n",
       " 3: 6131,\n",
       " 7: 6265,\n",
       " 2: 5958}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_dict  = dict(sorted(counter_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5923,\n",
       " 1: 6742,\n",
       " 2: 5958,\n",
       " 3: 6131,\n",
       " 4: 5842,\n",
       " 5: 5421,\n",
       " 6: 5918,\n",
       " 7: 6265,\n",
       " 8: 5851,\n",
       " 9: 5949}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary after sorting, we can infer the data is fairly balance we are not dealing with a imbalanced dataset\n",
    "counter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 9.871666666666666\n",
      "1 : 11.236666666666666\n",
      "2 : 9.93\n",
      "3 : 10.218333333333334\n",
      "4 : 9.736666666666666\n",
      "5 : 9.035\n",
      "6 : 9.863333333333333\n",
      "7 : 10.441666666666666\n",
      "8 : 9.751666666666667\n",
      "9 : 9.915\n"
     ]
    }
   ],
   "source": [
    "# percentage distribution\n",
    "for k in counter_dict:    \n",
    "    print(\"{} : {}\".format(k, 100* counter_dict[k]/sum(counter_dict.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn is more of your object-oriented programming \n",
    "### torch.nn.functional is more of just like functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as f \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in trainset:\n",
    "    k= data\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[0][0][0].shape\n",
    "# this represents the image width and height "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 column size \n"
     ]
    }
   ],
   "source": [
    "# essentially we are finding how many columns exist in that particular row \n",
    "# from this we get the number of columns\n",
    "no_of_columns =  (k[0][0][0][0].shape)[0]\n",
    "print(\"{} column size \".format(no_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 row size \n"
     ]
    }
   ],
   "source": [
    "# we are taking the first column fixing the rows as it is \n",
    "# from this we get the number of rows in our input image\n",
    "no_of_rows =(k[0][0][0][:][0].shape)[0]\n",
    "print(\"{} row size \".format(no_of_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 28 rows and 28 columns in our input image\n"
     ]
    }
   ],
   "source": [
    "print(\"we have {} rows and {} columns in our input image\".format(no_of_rows, no_of_columns)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # we will be initializing nn.Module\n",
    "        # super corresponds to nn.Module and all this is doing is running the initialization for nn.Module as well as whatever else we happen to put in it so when you inherit \n",
    "        #we inherit the attributes and methods, but the thing is the initialization method is not run so if we want to run the initialization method of the parent class where you\n",
    "        #are inheriting from we run super().__init()\n",
    "        super().__init__()\n",
    "        # our target here is to make 3 layers of 64 neurons \n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    # how do we want the data to be passed through the network    \n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = f.relu(self.fc2(x))\n",
    "        x = f.relu(self.fc3(x))\n",
    "        x = (self.fc4(x))\n",
    "        return f.log_softmax(x, dim =1)\n",
    "        \n",
    "net = Net()\n",
    "print(net)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before moving ahead we need to take into consideration is the loss and optimizer function. Loss function tells how much off your model is from the actual prediction. Optimizer adjusts the weights of the neurons based on the loss and gradients, it wants to adjust and optimize all of the possible weights that it can adjust in such a way to lower the loss slowly over time based on the learning rate we have used. \n",
    "\n",
    "` Optimizers: Adam optimizer` \n",
    "  Adam will be taking in two parameters one is the net.parameter this corresponds to everything that is adjustable in our model, there are things that don't necessarily have to be adjustable. \n",
    " This is true in the case of transfer learning examples where we have have a model pre-trained already on a specific domain and has learned the initial layers of the model has learnt all the low level features of the images such as edges, contrast etc. So the initial layers will be very good at smal and general types of image recognition task, and the laters layers will be learning closed domain specification/features. In transfer learning we freeze the initial layers create the bottleneck structure and the optimizer optimizes the weights only in the last few layers. \n",
    " \n",
    " ` Learning Rate `\n",
    "  Learning rate dictates the size of the step that your optimizer to get to the minima. So anytime you pass data through this normal neural network you get a loss, it is entirely possible to calculate what loss do we need for loss to be zero, to get perfect accuracy on the data we just sent, that is a simple mathematical operation. We need to determine the weights for our loss to be zero, to get perfect accuracy on the data we just sent. This is a simple mathematical operation which we don't want to perform as it would certainly lead to overfitting the model to our data points, a classical problem of the bias-variance problem which plagues every machine learning algorithm. So we use the optimizer to optimize, lower the loss but only take some certain size steps and then over time as we take those steps and the changes that were made basically based on,these steps will get overwritten as we are passing our data in batches. We need to optimize out learning rate because if we take a higher learning rate/step the model will never converge. If we take smaller steps it will never reach the global minima. \n",
    "  \n",
    "  ## But there is really no way to say that the model will reach the global minima( basically the perfect size step to reach the global minima) \n",
    "  ` so what we do in more generally more complicated tasks is we do what's called a decaying learning rate and the way this works is it starts off with these gigantic steps but over time the learning rate decreases `\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4166, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2331, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001) # 1e-03 \n",
    "\n",
    "EPOCHS = 3 \n",
    "\n",
    "for epochs in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of featuresets and labels \n",
    "        x, y = data \n",
    "        net.zero_grad()\n",
    "        output = net(x.view(-1,28*28))\n",
    "        loss = f.nll_loss(output, y)\n",
    "        # backpropogating the weights to each neuron adjusting the weights relative to how much each neuron contributed to the loss\n",
    "        loss.backward()\n",
    "        # this will adjust the weights for us \n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` We need to take extra precautions when we are deciding what loss function we have to use in our neural netowork. There are two major ways to calculate loss in a model `\n",
    "* One is based on ONE HOT ENCODING (one hot vectors)\n",
    "* The other is the normal probability distribution(pdf) vector like in this case. \n",
    "\n",
    "Our goal mostly is to use one hot encoded vectors. \n",
    "\n",
    "` If our input labels are one hot encoded it is safer to say that we should be using MSE loss (generally)`\n",
    "\n",
    "#### So it is safe to say that if your data is a scalar value like tensor(4) and not tensor( [ 0  0  0  1  0  0  0  0  0  0 ] ) just use NLL loss but if your data is a one hot vector use MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.988\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0 \n",
    "with torch.no_grad() :\n",
    "    for data in trainset :\n",
    "        x, y = data\n",
    "        output = net(x.view(-1,784))\n",
    "        for idx , i in enumerate(output):\n",
    "            # argmax returns the position of the tensor having the highest  \n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct +=1 \n",
    "            total +=1 \n",
    "print(\"Accuracy :\",round(correct/total,3))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Tensor.item() to get a Python number from a tensor containing a single value `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of training data is : 0.988\n"
     ]
    }
   ],
   "source": [
    "# Alternative and faster way to find the accuracy \n",
    "correct = 0 \n",
    "total = 0 \n",
    "with torch.no_grad() :\n",
    "    for data in trainset:\n",
    "        x,y = data \n",
    "        output = net(x.view(-1,784))\n",
    "        correct += output.argmax(dim =1).eq(y).sum().item()\n",
    "        total += len(y)\n",
    "\n",
    "print(f\"Accuracy of training data is : {round(correct/total,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of testing data is : 0.971\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_loss = 0 \n",
    "total =0\n",
    "correct = 0 \n",
    "for data in testset:\n",
    "    x, y= data \n",
    "    output = net(x.view (-1, 784))\n",
    "    correct += output.argmax(dim=1).eq(y).sum().item()\n",
    "    total += len(y)\n",
    "print(f\"Accuracy of testing data is : {round(correct/total,3)}\")    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpytorch",
   "language": "python",
   "name": "cpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
